ssh ghpcc06.umassrc.org -l tm94d

//8 cores 2Gigs per core, 1 host
bsub -n 8 -q long -W 8:00 -R "rusage[mem=2000]" -R "span[hosts=1]" < bwa_script.sh

//bsub -n 1000 -q long -W 300:00 -R "span[hosts=1]" < ~/SSRTWG/Project_0/code/naa_om_hpcc_script.sh
bsub -q long -R
#convert line endings
dos2unix ~/.R/Makevars

//interactive session with 4-hour runtime with 10G of memory (needed to install wham because of memory needs)
bsub -q interactive -R rusage[mem=10000] -W 4:00 -Is /bin/bash

//see what jobs you have running
bjobs -u tm94d
bjobs -u tm94d | wc -l


module avail //lists available modules

//most recent version of gcc
//module load gcc/11.2.0
//module load R/4.1.4

module load gcc/8.1.0
module load R/4.0.4_gcc
module load libjpeg-turbo/2.0.2
module load openmpi/4.0.1

//how much disk space
 df -h /home/tm94d

//queues
bqueues

//initiate an interactive R session
bsub -q interactive -R rusage[mem=10000] -W 4:00 -Is /bin/bash
module load gcc/8.1.0
module load R/4.0.4_gcc
module load libjpeg-turbo/2.0.2
cd ~/SSRTWG
R
.libPaths("~/Rlib/")
library(wham)
library(here)
/////////////////////////////

.libPaths()

.libPaths("~/Rlib/")
install.packages("TMB", lib = "~/Rlib/", repos='http://cran.us.r-project.org')
install.packages("remotes", lib = "~/Rlib/", repos='http://cran.us.r-project.org')
install.packages(c("here","snowfall"), lib = "~/Rlib/", repos='http://cran.us.r-project.org')
remotes::install_github("timjmiller/wham", dependencies=TRUE, ref="77bbd94")
remotes::install_github("timjmiller/wham", dependencies=FALSE, ref="77bbd94")

//Just some initial explorations here
//bsub -n 485 -q long -W 48:00 -R "rusage[mem=5000]" -J naa_om_sim_7 < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh
//bsub -n 10 -q long -W 0:15 -R "rusage[mem=5000]" -J naa_om_sim_7 < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh

//bsub -n 1 -q short -W 0:15 -R "rusage[mem=5000]" -J naa_om_sim_7_short < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 2 2 1 20"

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 3 3 1 20"

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 3 3 1 20"

//kill all jobs with job name
bkill -J naa_om_sim

//how many files in the current directory
ls | wc -l

//in ~/logs, number of log files for jobs that completed successfully
grep -rn --include=*.log -e "Success" ~/logs | wc -l

//count log files without "Success" in them (they didn't finish in the 2 hour limit)
cd ~/logs
grep -rn --include=*.log -L "Success" ~/logs | wc -l
//remove those files
grep -rn --include=*.log -L "Success" | xargs rm


//current approach for NAA_om sims on UMass hpcc. 
//bash script to submit seperate jobs to the cluster: naa_om_hpcc_bash_bsub.sh
// This file takes arguments: first_sim last_sim first_om last_om
// The script loops over the sims and oms to make each job submission looks like this:
// bsub -n $7 -q short -W 2:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh $this_sim $this_sim $this_om $this_om $5 $6"
// $7 is the number of cores (usually 20) last two arguments are the first and last estimating model (e.g. 1 20 for the first 20)
// this will set up a job using $7 processors on the short queue with a 2 hour time limit, write report to a a logfile in ~/logs, use 5GB per process, all on 1 blade/node and run the bash script naa_om_hpcc_args.sh
// The bash script statement  which includes necessary modules and for each job loops over the 20 estimating models and makes separate calls:
// Rscript --vanilla ~/SSRTWG/Project_0/code/naa_om_hpcc_script.R $sim $om $em &
// which will generate the simulated data from the appropriate operating model and fit the proper estimating model
// and write results to a RDS file in  ~/SSRTWG/Project_0/results/naa_om
// Users of the UMass HPCC are limited to a maximum 512 processors and maximum of 10000 job submissions at a time, but they prefer it if you keep it around 1000
// Jobs with 20 cores and 20 estimating models seem to usually take 10-15 minutes each so it should take about 0.5 days to get through ~1000 jobs
//The saved RDS file is a list with length = number of ems and the element corresponding to the em being fit is possibly filled with results.
//All other list elements will be NULL as will the slot for the em if the model causes R to abort.

//test
//bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 1 1 1 1 1 20 20
//These are the real commands to run on the server
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 1 40 1 24 1 20 20
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 41 80 1 24 1 20 20
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 81 100 1 24 1 20 20
#done

//for jobs that didn't finish in time use R session and run 
// source("~/SSRTWG/Project_0/code/sim_management.R")
// x <- get_failed_jobs()
// make_redo_failed_jobs_file(x)
// run jobs written to Project_0/code/redo_failed_jobs.txt


//here sims 1-100 done in ~ 1 day.
//3 didn't finish lets do them on the large queue for a longer run time
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 68 68 20 20 1 20"
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 32 32 14 14 1 20"
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 46 46 9 9 1 20"
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 93 93 2 2 1 20"

//delete aggregated files after they are copied over
//first see what directories
find -maxdepth 1 -type d -name "om_*" 
//then delete them
find -maxdepth 1 -type d -name "om_*" -exec rm -r {} \;

//Do M re operating model simulations
//bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 1 1 1 1 5 24 20
// bkill -J M_om*

bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 1 60 1 16 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 61 100 1 16 5 24 20
#done

//for jobs that didn't finish in time use R session and run 
// source("~/SSRTWG/Project_0/code/sim_management.R")
// x <- get_failed_jobs("M",5:24)
// make_redo_failed_jobs_file(x, "M")
// run jobs written to Project_0/code/redo_failed_jobs.txt

//Do selectivity re operating model simulations
bash ~/SSRTWG/Project_0/code/Sel_om_hpcc_bsub.sh 1 60 1 16 5 20 16
bash ~/SSRTWG/Project_0/code/Sel_om_hpcc_bsub.sh 1 60 1 16 25 28 4
bash ~/SSRTWG/Project_0/code/Sel_om_hpcc_bsub.sh 61 100 1 16 5 20 16
bash ~/SSRTWG/Project_0/code/Sel_om_hpcc_bsub.sh 61 100 1 16 25 28 4
// done

//Do catchability re operating model simulations
bash ~/SSRTWG/Project_0/code/q_om_hpcc_bsub.sh 1 60 1 16 5 20 16
bash ~/SSRTWG/Project_0/code/q_om_hpcc_bsub.sh 1 60 1 16 29 32 4
bash ~/SSRTWG/Project_0/code/q_om_hpcc_bsub.sh 61 100 1 16 5 20 16
bash ~/SSRTWG/Project_0/code/q_om_hpcc_bsub.sh 61 100 1 16 29 32 4
//done 

// x <- get_failed_jobs("q",5:20)
// make_redo_failed_jobs_file(x, "q")
// run jobs written to Project_0/code/redo_failed_jobs.txt

