//8 cores 2Gigs per core, 1 host
bsub -n 8 -q long -W 8:00 -R "rusage[mem=2000]" -R "span[hosts=1]" < bwa_script.sh

//bsub -n 1000 -q long -W 300:00 -R "span[hosts=1]" < ~/SSRTWG/Project_0/code/naa_om_hpcc_script.sh
bsub -q long -R
#convert line endings
dos2unix ~/.R/Makevars

//interactive session with 4-hour runtime with 10G of memory (needed to install wham because of memory needs)
bsub -q interactive -R rusage[mem=10000] -W 4:00 -Is /bin/bash

//see what jobs you have running
bjobs -u tm94d


module avail //lists available modules

//most recent version of gcc
//module load gcc/11.2.0
//module load R/4.1.4

module load gcc/8.1.0
module load R/4.0.4_gcc
module load libjpeg-turbo/2.0.2
module load openmpi/4.0.1

//how much disk space
 df -h /home/tm94d

//queues
bqueues


#start R
R

.libPaths()

install.packages("TMB", lib = "~/Rlib/", repos='http://cran.us.r-project.org')
install.packages("remotes", lib = "~/Rlib/", repos='http://cran.us.r-project.org')
install.packages(c("here","snowfall"), lib = "~/Rlib/", repos='http://cran.us.r-project.org')
.libPaths("~/Rlib/")
remotes::install_github("timjmiller/wham", dependencies=TRUE, ref="77bbd94")
remotes::install_github("timjmiller/wham", dependencies=FALSE, ref="77bbd94")

//Just some initial explorations here
//bsub -n 485 -q long -W 48:00 -R "rusage[mem=5000]" -J naa_om_sim_7 < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh
//bsub -n 10 -q long -W 0:15 -R "rusage[mem=5000]" -J naa_om_sim_7 < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh

//bsub -n 1 -q short -W 0:15 -R "rusage[mem=5000]" -J naa_om_sim_7_short < ~/SSRTWG/Project_0/code/naa_om_hpcc_7_7.sh

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 2 2 1 20"

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 3 3 1 20"

//bsub -n 20 -q short -W 2:00 -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 7 7 3 3 1 20"

//kill all jobs with job name
bkill -J naa_om_sim

//how many files in the current directory
ls | wc -l

//current approach for NAA_om sims on UMass hpcc. 
//bash script to submit seperate jobs to the cluster: naa_om_hpcc_bash_bsub.sh
// This file takes arguments: first_sim last_sim first_om last_om
// The script loops over the sims and oms to make each job submission looks like this:
// bsub -n $7 -q short -W 2:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_om_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh $this_sim $this_sim $this_om $this_om $5 $6"
// $7 is the number of cores (usually 20) last two arguments are the first and last estimating model (e.g. 1 20 for the first 20)
// this will set up a job using $7 processors on the short queue with a 2 hour time limit, write report to a a logfile in ~/logs, use 5GB per process, all on 1 blade/node and run the bash script naa_om_hpcc_args.sh
// The bash script statement  which includes necessary modules and for each job loops over the 20 estimating models and makes separate calls:
// Rscript --vanilla ~/SSRTWG/Project_0/code/naa_om_hpcc_script.R $sim $om $em &
// which will generate the simulated data from the appropriate operating model and fit the proper estimating model
// and write results to a RDS file in  ~/SSRTWG/Project_0/results/naa_om
// Users of the UMass HPCC are limited to a maximum 512 processors and maximum of 10000 job submissions at a time, but they prefer it if you keep it around 1000
// Jobs with 20 cores and 20 estimating models seem to usually take 10-15 minutes each so it should take about 0.5 days to get through ~1000 jobs
//The saved RDS file is a list with length = number of ems and the element corresponding to the em being fit is possibly filled with results.
//All other list elements will be NULL as will the slot for the em if the model causes R to abort.

//These are the real commands to run on the server
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 1 40 1 24 1 20 20
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 41 80 1 24 1 20 20
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 81 100 1 24 1 20 20
//here sims 1-100 done in ~ 1 day.
//3 didn't finish lets do them on the large queue for a longer run time
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 68 68 20 20 1 20"
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 32 32 14 14 1 20"
bsub -n 20 -q large -W 24:00 -o ~/logs/naa_logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J naa_sim "bash ~/SSRTWG/Project_0/code/naa_om_hpcc_args.sh 46 46 9 9 1 20"


//in ~/logs, number of log files for jobs that completed successfully
grep -rn --include=logfile.* -e "Success" | wc -l

//count log files without "Success" in them (they didn't finish in the 2 hour limit)
cd ~/logs
grep -rn --include=logfile.* -L "Success" | wc -l
//remove those files
grep -rn --include=logfile.* -L "Success" | xargs rm

//jobs didn't finish in time. Let's change the requested time limit to the max in naa_om_hpcc_bash_bsub.sh and rerun
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 4 4 17 17 1 20 20
bash ~/SSRTWG/Project_0/code/naa_om_hpcc_bsub.sh 6 6 12 12 1 20 20

//Do M re operating model simulations
//bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 1 1 1 1 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 1 20 1 16 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 21 60 1 16 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 61 100 1 16 5 24 20
//jobs didn't finish in time. Let's change the requested time limit to the max in naa_om_hpcc_bash_bsub.sh and rerun
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 35 35 15 15 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 78 78 14 14 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 37 37 13 13 5 24 20
bash ~/SSRTWG/Project_0/code/M_om_hpcc_bsub.sh 6 6 1 1 5 24 20
//Still didn't finish. Let's put it on the large queue for a longer run time
bsub -n 20 -q large -W 24:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J M_om_sim "bash ~/SSRTWG/Project_0/code/M_om_hpcc_args.sh 35 35 15 15 5 24"
bsub -n 20 -q large -W 24:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J M_om_sim "bash ~/SSRTWG/Project_0/code/M_om_hpcc_args.sh 78 78 14 14 5 24"
bsub -n 20 -q large -W 24:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J M_om_sim "bash ~/SSRTWG/Project_0/code/M_om_hpcc_args.sh 37 37 13 13 5 24" 20
bsub -n 20 -q large -W 24:00 -o ~/logs/logfile.%J -R "rusage[mem=5000]" -R "span[hosts=1]" -J M_om_sim "bash ~/SSRTWG/Project_0/code/M_om_hpcc_args.sh 6 6 1 1 5 24"

